{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 1.1, Implementing Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test parameters\n",
    "\n",
    "u_test = np.array([10,10])\n",
    "cov_test = np.array([10,0,0,10]).reshape([2,2])\n",
    "A_test = np.array([10,5,5,10]).reshape([2,2])\n",
    "b_test = np.array([400,400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Objective funtions and gradients\n",
    "\n",
    "#test objective function, vectorized input\n",
    "    #mean: vector of means of input vars\n",
    "    #cov: covariance matrix of input vars\n",
    "    #x: vector of inputs\n",
    "#returns scalar output\n",
    "def gaussian_objective(u, cov, x):\n",
    "    return -np.power(10,4) / (np.sqrt(np.power(2*np.pi,len(u))*np.linalg.det(cov))) *np.exp(-0.5 * np.dot(np.dot(np.subtract(x,u),np.linalg.inv(cov)), np.subtract(x,u)))\n",
    "    \n",
    "def gaussian_gradient(u, cov, x):\n",
    "    return -1*gaussian_objective(u,cov,x)*np.linalg.inv(cov)*np.subtract(x,u)\n",
    "\n",
    "#quadratic objective function\n",
    "    #A: positive definite matrix\n",
    "    #b: vector\n",
    "def quadratic_objective(A,b,x):\n",
    "    return 0.5 * np.subtract(np.dot(np.dot(x,A),x),np.dot(x,b))\n",
    "\n",
    "def quadratic_gradient(A,b,x):\n",
    "    return np.subtract(np.dot(A,x),b)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gradient descent\n",
    "\n",
    "def run_gradient_descent(func, deriv, x0, h, tol):\n",
    "    x = []\n",
    "    d = []\n",
    "    f = []\n",
    "    while 1:\n",
    "        dx0 = deriv(x0)\n",
    "        x.append(x0)\n",
    "        d.append(dx0)\n",
    "        \n",
    "        x1 = x0 - h*dx0\n",
    "        fx1 = func(x1)\n",
    "        fx0 = func(x0)\n",
    "        f.append(fx0)\n",
    "        if np.all(abs(fx1-fx0) < tol):\n",
    "            x.append(x1)\n",
    "            f.append(fx1)\n",
    "            break\n",
    "        x0 = x1\n",
    "    return x, d, f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([10, 10]),\n",
       "  array([ 35.,  35.]),\n",
       "  array([ 22.5,  22.5]),\n",
       "  array([ 28.75,  28.75]),\n",
       "  array([ 25.625,  25.625]),\n",
       "  array([ 27.1875,  27.1875]),\n",
       "  array([ 26.40625,  26.40625]),\n",
       "  array([ 26.796875,  26.796875]),\n",
       "  array([ 26.6015625,  26.6015625]),\n",
       "  array([ 26.69921875,  26.69921875]),\n",
       "  array([ 26.65039062,  26.65039062]),\n",
       "  array([ 26.67480469,  26.67480469]),\n",
       "  array([ 26.66259766,  26.66259766]),\n",
       "  array([ 26.66870117,  26.66870117]),\n",
       "  array([ 26.66564941,  26.66564941]),\n",
       "  array([ 26.66717529,  26.66717529]),\n",
       "  array([ 26.66641235,  26.66641235]),\n",
       "  array([ 26.66679382,  26.66679382]),\n",
       "  array([ 26.66660309,  26.66660309]),\n",
       "  array([ 26.66669846,  26.66669846]),\n",
       "  array([ 26.66665077,  26.66665077]),\n",
       "  array([ 26.66667461,  26.66667461]),\n",
       "  array([ 26.66666269,  26.66666269]),\n",
       "  array([ 26.66666865,  26.66666865]),\n",
       "  array([ 26.66666567,  26.66666567]),\n",
       "  array([ 26.66666716,  26.66666716])],\n",
       " [array([-250, -250]),\n",
       "  array([ 125.,  125.]),\n",
       "  array([-62.5, -62.5]),\n",
       "  array([ 31.25,  31.25]),\n",
       "  array([-15.625, -15.625]),\n",
       "  array([ 7.8125,  7.8125]),\n",
       "  array([-3.90625, -3.90625]),\n",
       "  array([ 1.953125,  1.953125]),\n",
       "  array([-0.9765625, -0.9765625]),\n",
       "  array([ 0.48828125,  0.48828125]),\n",
       "  array([-0.24414062, -0.24414062]),\n",
       "  array([ 0.12207031,  0.12207031]),\n",
       "  array([-0.06103516, -0.06103516]),\n",
       "  array([ 0.03051758,  0.03051758]),\n",
       "  array([-0.01525879, -0.01525879]),\n",
       "  array([ 0.00762939,  0.00762939]),\n",
       "  array([-0.0038147, -0.0038147]),\n",
       "  array([ 0.00190735,  0.00190735]),\n",
       "  array([-0.00095367, -0.00095367]),\n",
       "  array([ 0.00047684,  0.00047684]),\n",
       "  array([-0.00023842, -0.00023842]),\n",
       "  array([ 0.00011921,  0.00011921]),\n",
       "  array([ -5.96046448e-05,  -5.96046448e-05]),\n",
       "  array([  2.98023224e-05,   2.98023224e-05]),\n",
       "  array([ -1.49011612e-05,  -1.49011612e-05])],\n",
       " [array([ 950.,  950.]),\n",
       "  array([ 12075.,  12075.]),\n",
       "  array([ 4950.,  4950.]),\n",
       "  array([ 8121.875,  8121.875]),\n",
       "  array([ 6438.28125,  6438.28125]),\n",
       "  array([ 7255.6640625,  7255.6640625]),\n",
       "  array([ 6840.86914062,  6840.86914062]),\n",
       "  array([ 7046.74072266,  7046.74072266]),\n",
       "  array([ 6943.42346191,  6943.42346191]),\n",
       "  array([ 6994.98672485,  6994.98672485]),\n",
       "  array([ 6969.18125153,  6969.18125153]),\n",
       "  array([ 6982.07802773,  6982.07802773]),\n",
       "  array([ 6975.62814951,  6975.62814951]),\n",
       "  array([ 6978.85271609,  6978.85271609]),\n",
       "  array([ 6977.24033967,  6977.24033967]),\n",
       "  array([ 6978.04650459,  6978.04650459]),\n",
       "  array([ 6977.64341631,  6977.64341631]),\n",
       "  array([ 6977.844959,  6977.844959]),\n",
       "  array([ 6977.74418729,  6977.74418729]),\n",
       "  array([ 6977.79457305,  6977.79457305]),\n",
       "  array([ 6977.76938015,  6977.76938015]),\n",
       "  array([ 6977.78197659,  6977.78197659]),\n",
       "  array([ 6977.77567837,  6977.77567837]),\n",
       "  array([ 6977.77882748,  6977.77882748]),\n",
       "  array([ 6977.77725293,  6977.77725293]),\n",
       "  array([ 6977.7780402,  6977.7780402])])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_gradient_descent(lambda x: quadratic_objective(u_test, cov_test, x) ,lambda x: quadratic_gradient(A_test,b_test, x), np.array([10,10]), .1, .001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([10,\n",
       "  8.0,\n",
       "  6.4,\n",
       "  5.12,\n",
       "  4.096,\n",
       "  3.2768,\n",
       "  2.62144,\n",
       "  2.0971520000000003,\n",
       "  1.6777216000000004,\n",
       "  1.3421772800000003,\n",
       "  1.0737418240000003,\n",
       "  0.8589934592000003,\n",
       "  0.6871947673600002,\n",
       "  0.5497558138880001,\n",
       "  0.43980465111040007,\n",
       "  0.35184372088832006,\n",
       "  0.281474976710656,\n",
       "  0.22517998136852482,\n",
       "  0.18014398509481985,\n",
       "  0.14411518807585588,\n",
       "  0.11529215046068471,\n",
       "  0.09223372036854777,\n",
       "  0.07378697629483821,\n",
       "  0.05902958103587057,\n",
       "  0.04722366482869646,\n",
       "  0.037778931862957166],\n",
       " [20,\n",
       "  16.0,\n",
       "  12.8,\n",
       "  10.24,\n",
       "  8.192,\n",
       "  6.5536,\n",
       "  5.24288,\n",
       "  4.194304000000001,\n",
       "  3.3554432000000007,\n",
       "  2.6843545600000005,\n",
       "  2.1474836480000006,\n",
       "  1.7179869184000005,\n",
       "  1.3743895347200004,\n",
       "  1.0995116277760002,\n",
       "  0.8796093022208001,\n",
       "  0.7036874417766401,\n",
       "  0.562949953421312,\n",
       "  0.45035996273704965,\n",
       "  0.3602879701896397,\n",
       "  0.28823037615171176,\n",
       "  0.23058430092136942,\n",
       "  0.18446744073709553,\n",
       "  0.14757395258967643,\n",
       "  0.11805916207174114,\n",
       "  0.09444732965739291],\n",
       " [100,\n",
       "  64.0,\n",
       "  40.96000000000001,\n",
       "  26.2144,\n",
       "  16.777216,\n",
       "  10.73741824,\n",
       "  6.871947673600001,\n",
       "  4.398046511104002,\n",
       "  2.8147497671065613,\n",
       "  1.801439850948199,\n",
       "  1.1529215046068475,\n",
       "  0.7378697629483825,\n",
       "  0.47223664828696477,\n",
       "  0.3022314549036574,\n",
       "  0.19342813113834073,\n",
       "  0.12379400392853807,\n",
       "  0.07922816251426434,\n",
       "  0.050706024009129186,\n",
       "  0.03245185536584268,\n",
       "  0.020769187434139313,\n",
       "  0.013292279957849162,\n",
       "  0.008507059173023463,\n",
       "  0.0054445178707350165,\n",
       "  0.00348449143727041,\n",
       "  0.002230074519853063,\n",
       "  0.0014272476927059603])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_gradient_descent(lambda x: x**2 ,lambda x: 2*x, 10, .1, .001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def central_difference(func, step,x):\n",
    "    return (func(x+0.5*step) - func(x-0.5*step))/step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_objective(theta, X, Y):\n",
    "    return np.sum((np.dot(X,theta) - Y)**2)\n",
    "\n",
    "def batch_gradient(theta, X, Y):\n",
    "    return (2*np.dot(X.T,np.dot(X,theta)-Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,3) and (2,) not aligned: 3 (dim 1) != 2 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b09c3481e41e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtestTheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mbatch_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestTheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mbatch_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestTheta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-40c01ca68881>\u001b[0m in \u001b[0;36mbatch_objective\u001b[0;34m(theta, X, Y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatch_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbatch_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (2,3) and (2,) not aligned: 3 (dim 1) != 2 (dim 0)"
     ]
    }
   ],
   "source": [
    "m= 2\n",
    "p = 3\n",
    "testX = np.arange(m*p).reshape([m,p])\n",
    "testY = np.arange(p)\n",
    "testTheta = np.ones(m)\n",
    "\n",
    "print batch_objective(testTheta,testX,testY)\n",
    "print batch_gradient(testTheta,testX,testY)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n",
      "(100, 1)\n"
     ]
    }
   ],
   "source": [
    "import pylab as pl\n",
    "\n",
    "def getData():\n",
    "    \n",
    "    # load the fitting data for X and y and return as elements of a tuple\n",
    "    # X is a 100 by 10 matrix and y is a vector of length 100\n",
    "    # Each corresponding row for X and y represents a single data sample\n",
    "\n",
    "    X = pl.loadtxt('data/fittingdatap1_x.txt')\n",
    "    y = pl.loadtxt('data/fittingdatap1_y.txt', ndmin=2)\n",
    "\n",
    "    return (X,y) \n",
    "\n",
    "X_fitting, Y_fitting = getData();\n",
    "\n",
    "\n",
    "print np.shape(X_fitting)\n",
    "print np.shape(Y_fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = run_gradient_descent(lambda theta: batch_objective(theta, X_fitting, Y_fitting),lambda theta: batch_gradient(theta, X_fitting, Y_fitting), np.zeros((10,1)),0.000001,.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.5032541 ]\n",
      " [ -2.3367948 ]\n",
      " [ -6.31670032]\n",
      " [  6.81230107]\n",
      " [ -1.06337989]\n",
      " [  6.67469398]\n",
      " [  3.4118044 ]\n",
      " [ -0.45573592]\n",
      " [-12.94593466]\n",
      " [ 15.73289812]]\n",
      "8333.21421118\n",
      "[[  1.32516220e-10]\n",
      " [ -1.83476345e-10]\n",
      " [  2.82170731e-10]\n",
      " [  5.51381163e-12]\n",
      " [  6.62225830e-11]\n",
      " [ -1.36196832e-10]\n",
      " [  3.18891580e-11]\n",
      " [  7.22977234e-12]\n",
      " [ -4.66116035e-11]\n",
      " [ -5.33333377e-10]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "w_opt = np.dot(np.dot(np.linalg.inv(np.dot(X_fitting.T,X_fitting)), X_fitting.T), Y_fitting)\n",
    "print w_opt\n",
    "print batch_objective(w_opt, X_fitting, Y_fitting)\n",
    "print batch_gradient(w_opt,X_fitting, Y_fitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results_opt = run_gradient_descent(lambda theta: batch_objective(theta, X_fitting, Y_fitting),lambda theta: batch_gradient(theta, X_fitting, Y_fitting), w_opt,0.001,.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.50712932e-03]\n",
      " [ -1.26490924e-03]\n",
      " [  1.59659337e-03]\n",
      " [ -1.15665882e-03]\n",
      " [ -4.48456272e-04]\n",
      " [ -2.77008092e-03]\n",
      " [ -4.97742454e-03]\n",
      " [ -9.35992122e-04]\n",
      " [  7.56091429e-04]\n",
      " [ -2.21866974e-05]]\n"
     ]
    }
   ],
   "source": [
    "print results[0][-1] - w_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "def stochastic_gradient_descent(func, deriv, X, Y, theta0, tau, k, tol):\n",
    "    ndata = np.shape(X)[0]\n",
    "    t = 0\n",
    "    #theta_list = []\n",
    "    err = []\n",
    "    while 1:\n",
    "        order = range(ndata)\n",
    "        np.random.shuffle(order)\n",
    "        theta0_copy = theta0.copy()\n",
    "        for i in order:\n",
    "            xx = X[i,:].reshape((1,10))\n",
    "            yy = Y[i].reshape((1,1))\n",
    "            step = (tau+t)**(-k)\n",
    "            d = deriv(theta0,xx,yy)\n",
    "            theta1 = theta0-step*d\n",
    "            theta0 = theta1\n",
    "            t += 1\n",
    "            if t > 100000:\n",
    "                print err[-10:]\n",
    "                raise RuntimeError\n",
    "        fx1 = func(theta1,X,Y)\n",
    "        fx0 = func(theta0_copy,X,Y)\n",
    "        err.append(abs(fx1-fx0))\n",
    "        if abs(fx1-fx0) < tol:\n",
    "            break\n",
    "    return theta1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  9.67679204e-05]\n",
      " [ -1.96062820e-05]\n",
      " [ -7.18805281e-05]\n",
      " [ -9.49854239e-05]\n",
      " [ -1.01177143e-04]\n",
      " [  7.92209659e-05]\n",
      " [ -1.02350916e-05]\n",
      " [  1.13831074e-04]\n",
      " [ -2.57617266e-05]\n",
      " [ -5.08723528e-05]]\n"
     ]
    }
   ],
   "source": [
    "results_sgd = stochastic_gradient_descent(batch_objective,batch_gradient, X_fitting, Y_fitting, w_opt, 100000000.,.75,.1)\n",
    "print results_sgd-w_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.16227766017e-05\n",
      "3.16227528846e-05\n",
      "3.16227291676e-05\n",
      "3.16227054506e-05\n",
      "3.16226817337e-05\n",
      "3.16226580168e-05\n",
      "3.16226342999e-05\n",
      "3.16226105831e-05\n",
      "3.16225868664e-05\n",
      "3.16225631496e-05\n"
     ]
    }
   ],
   "source": [
    "tau = 1000000\n",
    "k = .75\n",
    "t = 0\n",
    "for i in range(10):\n",
    "    print (t+tau)**(-k)\n",
    "    t += 1\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1.33944892e-03]\n",
      " [ -1.47637471e-03]\n",
      " [  1.85975256e-03]\n",
      " [ -1.38911654e-03]\n",
      " [ -4.16107113e-04]\n",
      " [ -3.05058698e-03]\n",
      " [ -5.78215266e-03]\n",
      " [ -1.27045697e-03]\n",
      " [  7.16496296e-04]\n",
      " [ -6.45513866e-05]]\n"
     ]
    }
   ],
   "source": [
    "results_sgd = stochastic_gradient_descent(batch_objective,batch_gradient, X_fitting, Y_fitting, np.zeros((10,1)), 100000000.,.75,.1)\n",
    "print results_sgd-w_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Problem 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZAAAAEPCAYAAABsj5JaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRgwQSVIjBogkqREDRJLUiAEiSWrk/wMoo4KfhCZC7AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1092cac90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.2748  1.5728  0.2885  0.1237 -0.81   -1.5123 -0.8655 -0.8766 -0.6274\n",
      " -0.4159  0.8383]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "\n",
    "def getData(ifPlotData=True):\n",
    "    # load the fitting data and (optionally) plot out for examination\n",
    "    # return the X and Y as a tuple\n",
    "\n",
    "    data = pl.loadtxt('data/curvefittingp2.txt')\n",
    "\n",
    "    X = data[0,:]\n",
    "    Y = data[1,:]\n",
    "\n",
    "    if ifPlotData:\n",
    "        plt.plot(X,Y,'o')\n",
    "        plt.xlabel('x')\n",
    "        plt.ylabel('y')\n",
    "        plt.show()\n",
    "\n",
    "    return (X,Y)\n",
    "\n",
    "X, Y = getData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
